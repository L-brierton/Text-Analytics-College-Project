{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 2\n",
    "COMPETITION TASK: \n",
    "\n",
    "+ Learn the classification model for training set with 5 categorical data from ['business', 'entertainment', 'politics', 'sport', 'tech'].\n",
    "\n",
    "+ Apply learned model to get the labels for \"testdata.csv\"\n",
    "\n",
    "## Team Members: \n",
    "Laura Brierton - 15317451, Clodagh Lalor - 13354426, Jeremy Schiff - student#, Peter Concannon - student#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk, json\n",
    "from wordcloud import WordCloud\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we started by importing our datasets\n",
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_testdata = pd.read_csv('testdata.csv',sep='^',header=0)\n",
    "raw_trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Function to convert raw text to tokens\n",
    "def convert_tokens(rawtext, verbose=False):\n",
    "    # First: Tokenization\n",
    "    # start by removing hyphens to allow for better tokenization\n",
    "    rawtext = rawtext.replace('-', ' ')\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "    if (verbose):\n",
    "        print('Tokens:' + str(token_words[0:10]))\n",
    "    \n",
    "    # Second: Decapitalization \n",
    "    decap_token_words = [word.lower() for word in token_words]\n",
    "    if (verbose):\n",
    "        print('Decapitalized Tokens:' + str(decap_token_words[0:10]))\n",
    "    \n",
    "    # Third: Remove stop words\n",
    "    json_data=open('stopwords.json', encoding=\"utf8\").read()\n",
    "    stopwords_json = json.loads(json_data)\n",
    "    stopwords_json_en = set(stopwords_json['en'])\n",
    "    stopwords_nltk_en = set(stopwords.words('english'))\n",
    "    # Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "    stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)\n",
    "\n",
    "    \n",
    "    rmsw_token_words = ([word for word in decap_token_words if word.lower() not in stoplist_combined])\n",
    "    if (verbose):\n",
    "        print('Stopwords removed:' + str(rmsw_token_words[0:20]))\n",
    "    \n",
    "    ## Fouth: remove CAP words\n",
    "    rmcap_token_words =[]\n",
    "    for word in rmsw_token_words:\n",
    "        if word.isupper():\n",
    "            rmcap_token_words.append(word.title())\n",
    "        else:\n",
    "            rmcap_token_words.append(word)\n",
    "    if (verbose):\n",
    "        print('CAPITALIZED removed:' + str(rmcap_token_words[0:20]))\n",
    "        \n",
    "     ## Fifth : Remove salutation\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "    if (verbose):\n",
    "        print('Salutation removed:' + str(rmsalu_token_words[0:20]))\n",
    "        \n",
    "     ## Sixth: Remove words containing numbers\n",
    "    rmnb_token_words = ([word for word in rmsalu_token_words if not re.search(r\"\\d+\", word)])\n",
    "    if (verbose):\n",
    "        print('Number removed: ' + str(rmnb_token_words[0:20]))\n",
    "        \n",
    "    ## define transfer tag function:\n",
    "    def transfer_tag(treebank_tag):\n",
    "        if treebank_tag.startswith('j' or 'J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('v' or 'V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('n' or 'N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('r' or 'R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return 'n'\n",
    "    \n",
    "    ## Seventh: Lemmatization - this is the issue step\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rmnb_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        if not wtag:\n",
    "            lemma_words.extend([word])\n",
    "        ##please note we had to hardcode the following words in due to an error with word net\n",
    "        elif word == \"boss\":\n",
    "            lemma_words.extend([(word)])\n",
    "        elif word == \"gamers\":\n",
    "            lemma_words.extend([(\"gamer\")])\n",
    "        else:\n",
    "            lemma_words.extend([wnl.lemmatize(word, wtag)]) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    \n",
    "    ## RETURN\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "      <td>[french, boss, leave, eads, french, head, euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[gamer, drive, high, definition, tv, film, gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[stalemate, pension, strike, talk, talk, aim, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[johnny, denise, lose, passport, johnny, vaugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[tautou, star, da, vinci, film, french, actres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Media seek Jackson 'juror' notes Reporters cov...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[medium, seek, jackson, juror, note, reporter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror film heads US box office A low-budget h...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[horror, film, head, box, office, low, budget,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerr frustrated at victory margin Republic of ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[kerr, frustrate, victory, margin, republic, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US casino 'tricks' face ban in UK Controversia...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[casino, trick, face, ban, uk, controversial, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Klinsmann issues Lehmann warning Germany coach...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[klinsmann, issue, lehmann, warn, germany, coa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category  \\\n",
       "0  French boss to leave EADS The French co-head o...       business   \n",
       "1  Gamers could drive high-definition TV, films, ...           tech   \n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics   \n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment   \n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment   \n",
       "5  Media seek Jackson 'juror' notes Reporters cov...  entertainment   \n",
       "6  Horror film heads US box office A low-budget h...  entertainment   \n",
       "7  Kerr frustrated at victory margin Republic of ...          sport   \n",
       "8  US casino 'tricks' face ban in UK Controversia...       politics   \n",
       "9  Klinsmann issues Lehmann warning Germany coach...          sport   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [french, boss, leave, eads, french, head, euro...  \n",
       "1  [gamer, drive, high, definition, tv, film, gam...  \n",
       "2  [stalemate, pension, strike, talk, talk, aim, ...  \n",
       "3  [johnny, denise, lose, passport, johnny, vaugh...  \n",
       "4  [tautou, star, da, vinci, film, french, actres...  \n",
       "5  [medium, seek, jackson, juror, note, reporter,...  \n",
       "6  [horror, film, head, box, office, low, budget,...  \n",
       "7  [kerr, frustrate, victory, margin, republic, i...  \n",
       "8  [casino, trick, face, ban, uk, controversial, ...  \n",
       "9  [klinsmann, issue, lehmann, warn, germany, coa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we next create a dataframe that contained the content category and bag of words for each document\n",
    "df_handle = raw_trainset.copy()\n",
    "df_handle[\"Tokens\"] = df_handle.apply(lambda row: convert_tokens(row[\"content\"]), axis=1)\n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert tokens for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eminem secret gig venue revealed Rapper Eminem...</td>\n",
       "      <td>[eminem, secret, gig, venue, reveal, rapper, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Munster Cup tie switched to Spain Munster's He...</td>\n",
       "      <td>[munster, cup, tie, switch, spain, munster, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eastwood's Baby scoops top Oscars Clint Eastwo...</td>\n",
       "      <td>[eastwood, baby, scoop, top, oscar, clint, eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spears seeks aborted tour payment Singer Britn...</td>\n",
       "      <td>[spear, seek, aborted, tour, payment, singer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gadget show heralds MP3 Christmas Partners of ...</td>\n",
       "      <td>[gadget, show, herald, christmas, partner, lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US Airways staff agree to pay cut A union repr...</td>\n",
       "      <td>[airways, staff, agree, pay, cut, union, repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Blunkett row should end - Blair Prime Minister...</td>\n",
       "      <td>[blunkett, row, end, blair, prime, minister, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nasdaq planning $100m share sale The owner of ...</td>\n",
       "      <td>[nasdaq, plan, share, sale, owner, technology,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gallery unveils interactive tree A Christmas t...</td>\n",
       "      <td>[gallery, unveils, interactive, tree, christma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Escaped prisoner report ordered First Minister...</td>\n",
       "      <td>[escaped, prisoner, report, order, minister, j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Eminem secret gig venue revealed Rapper Eminem...   \n",
       "1  Munster Cup tie switched to Spain Munster's He...   \n",
       "2  Eastwood's Baby scoops top Oscars Clint Eastwo...   \n",
       "3  Spears seeks aborted tour payment Singer Britn...   \n",
       "4  Gadget show heralds MP3 Christmas Partners of ...   \n",
       "5  US Airways staff agree to pay cut A union repr...   \n",
       "6  Blunkett row should end - Blair Prime Minister...   \n",
       "7  Nasdaq planning $100m share sale The owner of ...   \n",
       "8  Gallery unveils interactive tree A Christmas t...   \n",
       "9  Escaped prisoner report ordered First Minister...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [eminem, secret, gig, venue, reveal, rapper, e...  \n",
       "1  [munster, cup, tie, switch, spain, munster, he...  \n",
       "2  [eastwood, baby, scoop, top, oscar, clint, eas...  \n",
       "3  [spear, seek, aborted, tour, payment, singer, ...  \n",
       "4  [gadget, show, herald, christmas, partner, lov...  \n",
       "5  [airways, staff, agree, pay, cut, union, repre...  \n",
       "6  [blunkett, row, end, blair, prime, minister, t...  \n",
       "7  [nasdaq, plan, share, sale, owner, technology,...  \n",
       "8  [gallery, unveils, interactive, tree, christma...  \n",
       "9  [escaped, prisoner, report, order, minister, j...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle_test = raw_testdata.copy()\n",
    "df_handle_test_onhold = raw_testdata.copy() #on hold until the end of the document\n",
    "df_handle_test[\"Tokens\"] = df_handle_test.apply(lambda row: convert_tokens(row[\"content\"]), axis=1)\n",
    "df_handle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continued the tokenizing process and created entries that contain only the noun or only the adjective tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generalization of the extraction\n",
    "def extract_pos_tokens(tokens, pos):\n",
    "    # helper for list comprehension\n",
    "    def is_pos(treebank_tag):\n",
    "        if treebank_tag.startswith(pos):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return [word for (word, tag) in nltk.pos_tag(tokens) if is_pos(tag)]\n",
    "\n",
    "#Specific noun instance\n",
    "def extract_noun_tokens(tokens):\n",
    "    # note that this does not include the \"or 'n'\" component which was both unnecessary and didnt work on my machine\n",
    "    # furthermore, it does not take noun to be the default\n",
    "    return extract_pos_tokens(tokens, 'N')\n",
    "    \n",
    "#Specific adjective instance\n",
    "def extract_adj_tokens(tokens):\n",
    "    # same idea as with nounds - the or 'j' is unneeded\n",
    "    return extract_pos_tokens(tokens, 'J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get noun and adjective tokens for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>noun_tokens</th>\n",
       "      <th>adjective_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "      <td>[french, boss, leave, eads, french, head, euro...</td>\n",
       "      <td>[boss, eads, defence, aerospace, group, statem...</td>\n",
       "      <td>[french, french, european, camus, camus, full,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[gamer, drive, high, definition, tv, film, gam...</td>\n",
       "      <td>[gamer, drive, definition, tv, film, game, tim...</td>\n",
       "      <td>[high, gear, high, short, popular, sound, expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[stalemate, pension, strike, talk, talk, aim, ...</td>\n",
       "      <td>[pension, strike, talk, talk, aim, strike, pen...</td>\n",
       "      <td>[stalemate, avert, national, public, deputy, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[johnny, denise, lose, passport, johnny, vaugh...</td>\n",
       "      <td>[denise, passport, johnny, vaughan, denise, va...</td>\n",
       "      <td>[johnny, saturday, big, plans, bring, real, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[tautou, star, da, vinci, film, french, actres...</td>\n",
       "      <td>[star, da, vinci, film, actress, star, film, a...</td>\n",
       "      <td>[tautou, french, tautou, female, brown, direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Media seek Jackson 'juror' notes Reporters cov...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[medium, seek, jackson, juror, note, reporter,...</td>\n",
       "      <td>[medium, seek, jackson, juror, note, reporter,...</td>\n",
       "      <td>[jackson, complete, potential, vital, potentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror film heads US box office A low-budget h...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[horror, film, head, box, office, low, budget,...</td>\n",
       "      <td>[horror, film, head, box, office, budget, horr...</td>\n",
       "      <td>[low, evil, dead, raimi, top, north, american,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerr frustrated at victory margin Republic of ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[kerr, frustrate, victory, margin, republic, i...</td>\n",
       "      <td>[frustrate, victory, margin, ireland, manager,...</td>\n",
       "      <td>[kerr, republic, brian, friendly, win, republi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US casino 'tricks' face ban in UK Controversia...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[casino, trick, face, ban, uk, controversial, ...</td>\n",
       "      <td>[casino, face, ban, uk, casino, trick, trade, ...</td>\n",
       "      <td>[trick, uk, controversial, ban, american, resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Klinsmann issues Lehmann warning Germany coach...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[klinsmann, issue, lehmann, warn, germany, coa...</td>\n",
       "      <td>[klinsmann, issue, coach, jurgen, klinsmann, g...</td>\n",
       "      <td>[germany, arsenal, alive, understudy, kahn, ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category  \\\n",
       "0  French boss to leave EADS The French co-head o...       business   \n",
       "1  Gamers could drive high-definition TV, films, ...           tech   \n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics   \n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment   \n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment   \n",
       "5  Media seek Jackson 'juror' notes Reporters cov...  entertainment   \n",
       "6  Horror film heads US box office A low-budget h...  entertainment   \n",
       "7  Kerr frustrated at victory margin Republic of ...          sport   \n",
       "8  US casino 'tricks' face ban in UK Controversia...       politics   \n",
       "9  Klinsmann issues Lehmann warning Germany coach...          sport   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [french, boss, leave, eads, french, head, euro...   \n",
       "1  [gamer, drive, high, definition, tv, film, gam...   \n",
       "2  [stalemate, pension, strike, talk, talk, aim, ...   \n",
       "3  [johnny, denise, lose, passport, johnny, vaugh...   \n",
       "4  [tautou, star, da, vinci, film, french, actres...   \n",
       "5  [medium, seek, jackson, juror, note, reporter,...   \n",
       "6  [horror, film, head, box, office, low, budget,...   \n",
       "7  [kerr, frustrate, victory, margin, republic, i...   \n",
       "8  [casino, trick, face, ban, uk, controversial, ...   \n",
       "9  [klinsmann, issue, lehmann, warn, germany, coa...   \n",
       "\n",
       "                                         noun_tokens  \\\n",
       "0  [boss, eads, defence, aerospace, group, statem...   \n",
       "1  [gamer, drive, definition, tv, film, game, tim...   \n",
       "2  [pension, strike, talk, talk, aim, strike, pen...   \n",
       "3  [denise, passport, johnny, vaughan, denise, va...   \n",
       "4  [star, da, vinci, film, actress, star, film, a...   \n",
       "5  [medium, seek, jackson, juror, note, reporter,...   \n",
       "6  [horror, film, head, box, office, budget, horr...   \n",
       "7  [frustrate, victory, margin, ireland, manager,...   \n",
       "8  [casino, face, ban, uk, casino, trick, trade, ...   \n",
       "9  [klinsmann, issue, coach, jurgen, klinsmann, g...   \n",
       "\n",
       "                                    adjective_tokens  \n",
       "0  [french, french, european, camus, camus, full,...  \n",
       "1  [high, gear, high, short, popular, sound, expe...  \n",
       "2  [stalemate, avert, national, public, deputy, p...  \n",
       "3  [johnny, saturday, big, plans, bring, real, fo...  \n",
       "4  [tautou, french, tautou, female, brown, direct...  \n",
       "5  [jackson, complete, potential, vital, potentia...  \n",
       "6  [low, evil, dead, raimi, top, north, american,...  \n",
       "7  [kerr, republic, brian, friendly, win, republi...  \n",
       "8  [trick, uk, controversial, ban, american, resp...  \n",
       "9  [germany, arsenal, alive, understudy, kahn, ge...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle[\"noun_tokens\"] = df_handle.apply(lambda row: extract_noun_tokens(row[\"Tokens\"]), axis=1)\n",
    "df_handle[\"adjective_tokens\"] = df_handle.apply(lambda row: extract_adj_tokens(row[\"Tokens\"]), axis=1)\n",
    "\n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get noun and adjective tokens for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>noun_tokens</th>\n",
       "      <th>adjective_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eminem secret gig venue revealed Rapper Eminem...</td>\n",
       "      <td>[eminem, secret, gig, venue, reveal, rapper, e...</td>\n",
       "      <td>[eminem, gig, venue, rapper, eminem, play, int...</td>\n",
       "      <td>[secret, saturday, river, friday, star, eminem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Munster Cup tie switched to Spain Munster's He...</td>\n",
       "      <td>[munster, cup, tie, switch, spain, munster, he...</td>\n",
       "      <td>[munster, cup, switch, spain, munster, quarter...</td>\n",
       "      <td>[tie, heineken, cup, final, real, sociedad, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eastwood's Baby scoops top Oscars Clint Eastwo...</td>\n",
       "      <td>[eastwood, baby, scoop, top, oscar, clint, eas...</td>\n",
       "      <td>[eastwood, baby, oscar, clint, eastwood, dolla...</td>\n",
       "      <td>[top, martin, scorsese, top, scorsese, hilary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spears seeks aborted tour payment Singer Britn...</td>\n",
       "      <td>[spear, seek, aborted, tour, payment, singer, ...</td>\n",
       "      <td>[seek, payment, singer, britney, sue, insuranc...</td>\n",
       "      <td>[spear, tour, spear, onyx, knee, seek, supreme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gadget show heralds MP3 Christmas Partners of ...</td>\n",
       "      <td>[gadget, show, herald, christmas, partner, lov...</td>\n",
       "      <td>[gadget, show, partner, love, tech, expert, pr...</td>\n",
       "      <td>[christmas, present, early, top, irish, irish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US Airways staff agree to pay cut A union repr...</td>\n",
       "      <td>[airways, staff, agree, pay, cut, union, repre...</td>\n",
       "      <td>[staff, pay, cut, union, represent, flight, at...</td>\n",
       "      <td>[airways, cut, 창, third, mechanic, cleaner, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Blunkett row should end - Blair Prime Minister...</td>\n",
       "      <td>[blunkett, row, end, blair, prime, minister, t...</td>\n",
       "      <td>[blunkett, end, blair, minister, tony, blair, ...</td>\n",
       "      <td>[prime, draw, lib, call, fast, nanny, budd, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nasdaq planning $100m share sale The owner of ...</td>\n",
       "      <td>[nasdaq, plan, share, sale, owner, technology,...</td>\n",
       "      <td>[nasdaq, plan, share, sale, owner, technology,...</td>\n",
       "      <td>[public, 창, full, public, nasdaq, cold, privat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gallery unveils interactive tree A Christmas t...</td>\n",
       "      <td>[gallery, unveils, interactive, tree, christma...</td>\n",
       "      <td>[gallery, tree, christmas, receive, text, mess...</td>\n",
       "      <td>[unveils, interactive, tree, unveil, london, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Escaped prisoner report ordered First Minister...</td>\n",
       "      <td>[escaped, prisoner, report, order, minister, j...</td>\n",
       "      <td>[prisoner, report, order, minister, jack, mcco...</td>\n",
       "      <td>[escaped, schizophrenic, high, snp, nicola, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Eminem secret gig venue revealed Rapper Eminem...   \n",
       "1  Munster Cup tie switched to Spain Munster's He...   \n",
       "2  Eastwood's Baby scoops top Oscars Clint Eastwo...   \n",
       "3  Spears seeks aborted tour payment Singer Britn...   \n",
       "4  Gadget show heralds MP3 Christmas Partners of ...   \n",
       "5  US Airways staff agree to pay cut A union repr...   \n",
       "6  Blunkett row should end - Blair Prime Minister...   \n",
       "7  Nasdaq planning $100m share sale The owner of ...   \n",
       "8  Gallery unveils interactive tree A Christmas t...   \n",
       "9  Escaped prisoner report ordered First Minister...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [eminem, secret, gig, venue, reveal, rapper, e...   \n",
       "1  [munster, cup, tie, switch, spain, munster, he...   \n",
       "2  [eastwood, baby, scoop, top, oscar, clint, eas...   \n",
       "3  [spear, seek, aborted, tour, payment, singer, ...   \n",
       "4  [gadget, show, herald, christmas, partner, lov...   \n",
       "5  [airways, staff, agree, pay, cut, union, repre...   \n",
       "6  [blunkett, row, end, blair, prime, minister, t...   \n",
       "7  [nasdaq, plan, share, sale, owner, technology,...   \n",
       "8  [gallery, unveils, interactive, tree, christma...   \n",
       "9  [escaped, prisoner, report, order, minister, j...   \n",
       "\n",
       "                                         noun_tokens  \\\n",
       "0  [eminem, gig, venue, rapper, eminem, play, int...   \n",
       "1  [munster, cup, switch, spain, munster, quarter...   \n",
       "2  [eastwood, baby, oscar, clint, eastwood, dolla...   \n",
       "3  [seek, payment, singer, britney, sue, insuranc...   \n",
       "4  [gadget, show, partner, love, tech, expert, pr...   \n",
       "5  [staff, pay, cut, union, represent, flight, at...   \n",
       "6  [blunkett, end, blair, minister, tony, blair, ...   \n",
       "7  [nasdaq, plan, share, sale, owner, technology,...   \n",
       "8  [gallery, tree, christmas, receive, text, mess...   \n",
       "9  [prisoner, report, order, minister, jack, mcco...   \n",
       "\n",
       "                                    adjective_tokens  \n",
       "0  [secret, saturday, river, friday, star, eminem...  \n",
       "1  [tie, heineken, cup, final, real, sociedad, sa...  \n",
       "2  [top, martin, scorsese, top, scorsese, hilary,...  \n",
       "3  [spear, tour, spear, onyx, knee, seek, supreme...  \n",
       "4  [christmas, present, early, top, irish, irish,...  \n",
       "5  [airways, cut, 창, third, mechanic, cleaner, fa...  \n",
       "6  [prime, draw, lib, call, fast, nanny, budd, ap...  \n",
       "7  [public, 창, full, public, nasdaq, cold, privat...  \n",
       "8  [unveils, interactive, tree, unveil, london, a...  \n",
       "9  [escaped, schizophrenic, high, snp, nicola, re...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle_test[\"noun_tokens\"] = df_handle_test.apply(lambda row: extract_noun_tokens(row[\"Tokens\"]), axis=1)\n",
    "df_handle_test[\"adjective_tokens\"] = df_handle_test.apply(lambda row: extract_adj_tokens(row[\"Tokens\"]), axis=1)\n",
    "\n",
    "df_handle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deconstruction - Wordclouds and Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decided to create a wordcloud for the entire corpus, to get an idea of the most common words and if there was any common pattern. We thought that it might help us to decide if there were any more pre-processing steps we needed to take before moving onto our Analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud function\n",
    "def wordcloudplot(tokens, name):\n",
    "    \n",
    "    text2 = ' '.join(tokens)\n",
    "\n",
    "    wordcloud = WordCloud(width=1600, height=800).generate(text2)\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    # save to file if filename given\n",
    "    if name:\n",
    "        wordcloud.to_file(name)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordcloudplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c653a8c38c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#saves wordcloud of all tokens as file. Please note, this word cloud is for all tokens not just noun tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwordcloudplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'img_wordcloud1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wordcloudplot' is not defined"
     ]
    }
   ],
   "source": [
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "\n",
    "#saves wordcloud of all tokens as file. Please note, this word cloud is for all tokens not just noun tokens\n",
    "wordcloudplot(joined_tokens, 'img_wordcloud1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our word cloud, we can see immediately that a lot of verbs are present, this will not necessarily help us with our classification step and so this influenced us to look at noun and adjective tokens instead, going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_tokens = [token for document in df_handle[\"noun_tokens\"] for token in document]\n",
    "adjective_tokens = [token for document in df_handle[\"adjective_tokens\"] for token in document]\n",
    "wordcloudplot(noun_tokens, 'img_wordcloud2.png')\n",
    "wordcloudplot(adjective_tokens, 'img_wordcloud3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, we wanted to look at the frequency of certain nouns and adjectives overall in the data with hopes that we could glean some information to aid classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_noun_tokens = [token for document in df_handle[\"noun_tokens\"] for token in document]\n",
    "word_frequency = nltk.FreqDist(joined_noun_tokens)\n",
    "word_frequency.plot(20, title='Twenty Most Common Nouns')\n",
    "\n",
    "joined_adjective_tokens = [token for document in df_handle[\"adjective_tokens\"] for token in document]\n",
    "word_frequency = nltk.FreqDist(joined_adjective_tokens)\n",
    "word_frequency.plot(20, title='Twenty Most Common Adjectives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nouns looks pretty useful with words like government, film, and game all likely being high indicators as to the category of the data. On the other hand, the adjectives seem much less useful with words like high, big, and good being so prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Most Common Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of our analysis involved looking at bigrams and trigrams, which in turn will be a huge help to us when it comes to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "\n",
    "bigram = ngrams(joined_tokens, 2)\n",
    "bi_frequencies = nltk.FreqDist(bigram)\n",
    "dict_items =list(dict(bi_frequencies).items())\n",
    "#make a dataframe of the bigrams and their frequencies\n",
    "df_bigramFreq = pd.DataFrame(dict_items, columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_bigramFreq = df_bigramFreq.reset_index(drop=True)\n",
    "#show only top five\n",
    "df_bigramFreq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the gram is noun gram or not\n",
    "def IsNounGram(ngram):\n",
    "    if ('-pron-' in ngram) or ('t' in ngram):\n",
    "        return False\n",
    "    \n",
    "    first_type = ('JJ','JJR','JJS','NN','NNS','NNP','NNPS')\n",
    "    second_type = ('NN','NNS','NNP','NNPS')\n",
    "    tags = nltk.pos_tag(ngram,lang='eng')\n",
    "    if (tags[0][1] in first_type) and (tags[1][1] in second_type):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##look only at top 50 for efficiency sake\n",
    "df_bigramFreq_filter = df_bigramFreq.copy().head(50)\n",
    "#creaete a new column which check for each bigram, which are noun grams , returns true or false\n",
    "df_bigramFreq_filter['noun_gram'] = df_bigramFreq_filter[\"bigram\"].map(lambda x : IsNounGram(x))\n",
    "#filter out those that are false\n",
    "df_bigramFreq_filter = df_bigramFreq_filter[df_bigramFreq_filter.noun_gram != False]\n",
    "df_bigramFreq_filter.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that our top five have changed a little (as has the entire dataframe). We were concerned with the (tell, BBC) bigram, and so decided that we needed to add an additional step of checking that this appeared in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check that these ngrams actually appear together in the text!\n",
    "def CheckWordInText(word, Text):\n",
    "    if word in Text.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all texts\n",
    "full_corpus =' '.join([document for document in df_handle[\"content\"]])\n",
    "# combine all the tokens\n",
    "for index, row in df_handle.iterrows():\n",
    "    full_corpus = full_corpus + df_handle['content'].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note this next cell is taking FOREVER, anyone have an alternative?\n",
    "#Jeremy: Does not seem terrible runtime to me, perhaps fixed when I fixed the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_bigram = df_bigramFreq_filter.copy()\n",
    "\n",
    "#true if exists\n",
    "exists_list = []\n",
    "for index, row in df_bigramFreq_filter.iterrows():\n",
    "    gram = row['bigram']\n",
    "    word = (' '.join(gram))\n",
    "    exists_list.append(CheckWordInText(word, full_corpus))\n",
    "\n",
    "#create new column for these    \n",
    "df_bigramFreq_filter['exist'] = exists_list    \n",
    "\n",
    "#delete those that are false\n",
    "df_real_bigram = df_bigramFreq_filter[df_bigramFreq_filter.exist != False]\n",
    "\n",
    "#show top 5\n",
    "df_real_bigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now do the same for trigrams. We decided to go no higher than n = 3, for the purposes of this corpus. WHY???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "trigram = ngrams(joined_tokens, 3)\n",
    "tri_frequencies = nltk.FreqDist(trigram)\n",
    "dict_items = list(dict(tri_frequencies).items())\n",
    "#make a dataframe of the trigrams and their frequencies\n",
    "df_trigramFreq = pd.DataFrame(dict_items, columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_trigramFreq = df_trigramFreq.reset_index(drop=True)\n",
    "\n",
    "#check for noun grams\n",
    "##look only at top 50 for efficiency sake\n",
    "df_trigramFreq_filter = df_trigramFreq.copy().head(50)\n",
    "#creaete a new column which check for each bigram, which are noun grams , returns true or false\n",
    "df_trigramFreq_filter['noun_gram'] = df_trigramFreq_filter[\"trigram\"].map(lambda x : IsNounGram(x))\n",
    "#filter out those that are false\n",
    "df_trigramFreq_filter = df_trigramFreq_filter[df_trigramFreq_filter.noun_gram != False]\n",
    "#printf(df_bigramFreq_filter.head(5))\n",
    "\n",
    "#check in text\n",
    "df_real_trigram = df_trigramFreq_filter.copy()\n",
    "\n",
    "exits_list = []\n",
    "for index, row in df_trigramFreq_filter.iterrows():\n",
    "    gram = row['trigram']\n",
    "    word = (' '.join(gram))\n",
    "    exits_list.append(CheckWordInText(word, full_corpus))\n",
    "    \n",
    "df_real_trigram['exist'] = exits_list\n",
    "df_real_trigram = df_real_trigram.loc[df_real_trigram.exist==True]\n",
    "\n",
    "\n",
    "#df_trigramFreq_filter.head(5)\n",
    "df_real_trigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on this...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Vectorising the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a subset of the training set to test later functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the sample output given shows only zeros but it is working\n",
    "merged_tokens = [\" \".join(x) for x in df_handle[\"noun_tokens\"]]\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_train = pd.DataFrame(tfidf_vectorizer.fit_transform(merged_tokens).todense(), columns = tfidf_vectorizer.get_feature_names())\n",
    "tfidf_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Vectorising the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_data = [\" \".join(x) for x in df_handle_test.Tokens]\n",
    "\n",
    "# Create a tfidf for the test data with the same dimensions (i.e. headers)\n",
    "# as the set above. necessary for comparing models\n",
    "tfidf_clean = tfidf_vectorizer.transform(test_clean_data)\n",
    "## For printing that tf-idf matrix, we convert it into dataframe\n",
    "tfidf_test = pd.DataFrame(tfidf_clean.toarray(),columns=[tfidf_vectorizer.get_feature_names()])\n",
    "tfidf_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = df_handle['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(unique,counts)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see this is a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Select Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Cross Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.1\n",
    "num_folds = 10\n",
    "model_tfidf, validation_tfidf, model_labels, validation_labels = train_test_split(tfidf_train, labels_train, test_size=validation_size, shuffle=True, stratify=labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAYES classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbayes = MultinomialNB()\n",
    "modelbayes.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "randomforest.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelkmeans = KMeans(n_clusters=5, init='k-means++', max_iter=200, n_init=100)\n",
    "modelkmeans.fit(model_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##I need to work on this further\n",
    "##https://appliedmachinelearning.blog/2018/01/18/conventional-approach-to-text-classification-clustering-using-k-nearest-neighbor-k-means-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes about 5-6 mins to complete\n",
    "k_values = [1,2]\n",
    "print(\"the range of k is \" + str(k_values))\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), {'n_neighbors':k_values}, scoring='accuracy', cv = num_folds)\n",
    "grid_search.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelknn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "modelknn.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this subset, the best value is k=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1, 1e2]\n",
    "grid_search = GridSearchCV(LogisticRegression(), {'C':c_values}, scoring='accuracy', cv = num_folds)\n",
    "grid_search.fit(model_tfidf, model_labels)\n",
    "\n",
    "modellr = LogisticRegression(**grid_search.best_params_)\n",
    "modellr.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Applying models to test set to estimate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_bayes = modelbayes.predict(validation_tfidf)\n",
    "predicted_probas_bayes = modelbayes.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_randomforest = randomforest.predict(validation_tfidf)\n",
    "predicted_probas_randomforest = randomforest.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_kmeans = modelkmeans.predict(validation_tfidf)\n",
    "#predicted_probas_kmeans = modelkmeans.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_knn = modelknn.predict(validation_tfidf)\n",
    "predicted_probas_knn = modelknn.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_lr = modellr.predict(validation_tfidf)\n",
    "predicted_probas_lr = modellr.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_bayes = accuracy_score(validation_labels, predicted_labels_bayes)\n",
    "print('Accuracy rate for NB model: {:0.2f}%'.format(Acc_bayes*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_ranfor = accuracy_score(validation_labels, predicted_labels_randomforest)\n",
    "print('Accuracy rate for RandomForest model: {:0.2f}%'.format(Acc_ranfor*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_kmeans = accuracy_score(validation_labels, predicted_labels_kmeans)\n",
    "print('Accuracy rate for K-Means model: {:0.2f}%'.format(Acc_kmeans*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_knn = accuracy_score(validation_labels, predicted_labels_knn)\n",
    "print('Accuracy rate for KNN model: {:0.2f}%'.format(Acc_knn*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_lr = accuracy_score(validation_labels, predicted_labels_lr)\n",
    "print('Accuracy rate for Logistic model: {:0.2f}%'.format(Acc_lr*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Evaluation and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Log Loss as our evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_bayes = log_loss(validation_labels, predicted_probas_bayes)\n",
    "print('Error rate for Logistic model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_bayes*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_ranfor = log_loss(validation_labels, predicted_probas_randomforest)\n",
    "print('Error rate for Logistic model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_ranfor*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a problem here something about the dimension of the probability matrix, tried normalising, didn't fix it\n",
    "log_loss_knn = log_loss(validation_labels, predicted_probas_knn)\n",
    "print('Error rate for Logistic model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_knn*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_lr = log_loss(validation_labels, predicted_probas_lr)\n",
    "print('Error rate for Logistic model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_lr*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both tests, Bayes seems to perform the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Applying Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels_bayes = modelbayes.predict(tfidf_test)\n",
    "predicted_test_labels_rf = randomforest.predict(tfidf_test)\n",
    "predicted_test_labels_knn = modelknn.predict(tfidf_test)\n",
    "predicted_test_labels_lr = modellr.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_handle_test_onhold[\"Pred Labels Bayes\"] = predicted_test_labels_bayes\n",
    "df_handle_test_onhold[\"Pred Labels RF\"] = predicted_test_labels_rf\n",
    "df_handle_test_onhold[\"Pred Labels KNN\"] = predicted_test_labels_knn\n",
    "df_handle_test_onhold[\"Pred Labels LR\"] = predicted_test_labels_lr\n",
    "df_handle_test_onhold.to_csv('predicted labels.csv')\n",
    "df_handle_test_onhold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
